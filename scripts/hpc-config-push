#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Copyright (C) 2020 EDF SA
# Contact:
#       CCN - HPC <dsp-cspit-ccn-hpc@edf.fr>
#       1, Avenue du General de Gaulle
#       92140 Clamart
#
# Authors: CCN - HPC <dsp-cspit-ccn-hpc@edf.fr>
#
# This file is part of hpc-config.
#
# hpc-config is free software: you can redistribute in and/or
# modify it under the terms of the GNU General Public License,
# version 2, as published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public
# License along with hpc-config. If not, see
# <http://www.gnu.org/licenses/>.

import os
import fnmatch
import argparse
import configparser
from io import StringIO
import tarfile
import sys
import subprocess
import logging
logger = logging.getLogger(__name__)
import tempfile
import glob
import shutil
import boto
import boto.s3
import boto.s3.connection
import paramiko
import stat
import socket
import warnings
import time
import hashlib
import yaml
from multiprocessing.dummy import Pool as ThreadPool
from multiprocessing import Pool
from datetime import datetime

_area_passwords_cache = {}

def conf_copy(src, dst, *, follow_symlinks=True):
    """Alternate copy function for shutil.copytree() in order to properly
       resolve and copy symlinks to directories. It is used to copy private
       files into the tmp directory. If the path is a directory, call copytree()
       with self as copy_function, otherwise (flat file) call copy2()."""
    if os.path.isdir(src):
        # if dst already exist, skip level by calling conf_copy() on src/*
        if os.path.exists(dst):
            for item in os.listdir(src):
                subsrc = os.path.join(src, item)
                subdst = os.path.join(dst, item)
                conf_copy(subsrc, subdst, follow_symlinks=follow_symlinks)
        else:
            shutil.copytree(src, dst,
                            symlinks=not follow_symlinks,
                            copy_function=conf_copy)
    else:
        shutil.copy2(src, dst, follow_symlinks=follow_symlinks)

class AppConf():
    """Runtime configuration class."""

    def __init__(self):

        self.debug = False
        self.conf_file = None
        self.cluster = None
        self.environment = None
        self.version = None

        self.mode = None

        ## Common parameters
        self.destination_root = None

        ## Posix Parameters
        self.posix_file_mode = None
        self.posix_dir_mode = None

        ## S3 parameters
        self.s3_access_key = None
        self.s3_secret_key = None
        self.s3_bucket_name = None
        self.s3_host = None
        self.s3_port = None

        ## SFTP parameters
        self.sftp_hosts = None
        self.sftp_username = None
        self.sftp_private_key = None

        # action

        self.full_tmp_cleanup = False
        self.list_environments = False

        # paths

        self.conf_puppet = None
        self.conf_hiera = None
        # This one is hard-coded, there is no configuration parameter to
        # change it since it would be irrelevant to change it.
        self.conf_environment = 'environment.conf'
        self.nodes_private = None
        self.dir_modules_generic = None
        self.dir_modules_private = None
        self.dir_manifests_generic = None
        self.dir_manifests_private = None
        self.dir_hieradata_generic = None
        self.dir_hieradata_private = None
        self.src_dir_hieradata_private = None
        self.dir_files_private = None
        self.src_dir_files_private = None


        self.dir_tmp = None
        self.dir_tmp_gen = None

    def dump(self):
        logger.debug("runtime configuration dump:")
        logger.debug("- debug: %s", str(self.debug))
        logger.debug("- conf_file: %s", str(self.conf_file))
        logger.debug("- cluster: %s", str(self.cluster))
        logger.debug("- environment: %s", str(self.environment))
        logger.debug("- version: %s", str(self.version))
        logger.debug("- mode: %s", str(self.mode))
        logger.debug("- destination_root: %s", str(self.destination_root))
        logger.debug("- destination: %s", str(self.destination))
        logger.debug("- areas: %s", str(self.areas))
        logger.debug("- dir_tmp: %s", str(self.dir_tmp))
        logger.debug("- conf_puppet: %s", str(self.conf_puppet))
        logger.debug("- conf_hiera: %s", str(self.conf_hiera))
        logger.debug("- nodes_private: %s", str(self.nodes_private))
        logger.debug("- dir_modules_generic: %s", str(self.dir_modules_generic))
        logger.debug("- dir_modules_private: %s", str(self.dir_modules_private))
        logger.debug("- dir_manifests_generic: %s", str(self.dir_manifests_generic))
        logger.debug("- dir_manifests_private: %s", str(self.dir_manifests_private))
        logger.debug("- dir_hieradata_generic: %s", str(self.dir_hieradata_generic))
        logger.debug("- src_dir_hieradata_private: %s", str(self.src_dir_hieradata_private))
        logger.debug("- src_dir_files_private: %s", str(self.src_dir_files_private))
        logger.debug("- posix_file_mode: %s", str(self.posix_file_mode))
        logger.debug("- posix_dir_mode: %s", str(self.posix_dir_mode))
        logger.debug("- s3_access_key: %s", str(self.s3_access_key))
        logger.debug("- s3_secret_key: %s", str(self.s3_secret_key))
        logger.debug("- s3_bucket_name: %s", str(self.s3_bucket_name))
        logger.debug("- s3_port: %s", str(self.s3_port))
        logger.debug("- s3_host: %s", str(self.s3_host))
        logger.debug("- sftp_hosts: %s", str(self.sftp_hosts))
        logger.debug("- sftp_username: %s", str(self.sftp_username))
        logger.debug("- sftp_private_key: %s", str(self.sftp_private_key))

    def archive_path(self, area):
        return os.path.join(self.dir_tmp_gen, area, 'puppet-config-environment.tar.xz')

    @property
    def conf_environment_gen(self):
        """Path where environment.conf is generated."""
        return os.path.join(self.dir_tmp_gen, self.conf_environment)

    @property
    def destination(self):
        return os.path.join(self.destination_root, self.environment, self.version)

conf = AppConf()            # global runtime configuration object
_sftp_host_directories = {} # global cache of remote directories

def parse_conf():
    """Parse configuration file and set runtime configuration accordingly.
       Here are defined default configuration file parameters."""
    defaults = StringIO(
      "[global]\n"
      "cluster = unknown\n"
      "environment = production\n"
      "version = latest\n"
      "mode = posix\n"
      "destination = /var/www/html/hpc-config\n"
      "areas = default\n"
      "[posix]\n"
      "file_mode = 644\n"
      "dir_mode = 755\n"
      "[s3]\n"
      "access_key = XXXXXXXX\n"
      "secret_key = YYYYYYYYYYYYYYYY\n"
      "bucket_name = system\n"
      "host = rgw.service.virtual\n"
      "port = 7480\n"
      "[sftp]\n"
      "hosts = localhost\n"
      "username = root\n"
      "private_key = /root/.ssh/id_rsa\n"
      "[paths]\n"
      "tmp = /tmp/puppet-config-push\n"
      "puppethpc = puppet-hpc\n"
      "privatedata = hpc-privatedata\n"
      "puppet_conf = ${privatedata}/puppet-config/${global:cluster}/puppet.conf\n"
      "hiera_conf = ${privatedata}/puppet-config/${global:cluster}/hiera.yaml\n"
      "nodes_private = ${privatedata}/puppet-config/${global:cluster}/cluster-nodes.yaml\n"
      "modules_generic = ${puppethpc}/puppet-config/cluster,${puppethpc}/puppet-config/modules,/usr/share/puppet/modules\n"
      "modules_private = ${privatedata}/puppet-config/${global:cluster}/modules\n"
      "manifests_generic = ${puppethpc}/puppet-config/manifests\n"
      "manifests_private = ${privatedata}/puppet-config/${global:cluster}/manifests\n"
      "hieradata_generic = ${puppethpc}/hieradata\n"
      "hieradata_private = ${privatedata}/hieradata\n"
      "files_private = ${privatedata}/files/${global:cluster}\n")
    parser = configparser.ConfigParser()
    parser._interpolation = configparser.ExtendedInterpolation()
    parser.readfp(defaults)
    parser.read(conf.conf_file)
    conf.cluster = parser.get('global', 'cluster')
    conf.environment = parser.get('global', 'environment')
    conf.version = parser.get('global', 'version')
    conf.mode = parser.get('global', 'mode')
    conf.destination_root = parser.get('global', 'destination')
    conf.areas = parser.get('global', 'areas').split(',')
    conf.main_area = conf.areas[0] # the main area is the first declared area
    conf.dir_tmp = parser.get('paths', 'tmp')
    conf.conf_puppet = parser.get('paths', 'puppet_conf')
    conf.conf_hiera = parser.get('paths', 'hiera_conf')
    if parser.has_option('paths', 'facts_private'):
        logger.warning('configuration facts_private is deprecated, use nodes_private')
    conf.nodes_private = parser.get('paths', 'nodes_private')
    conf.dir_modules_generic = parser.get('paths', 'modules_generic').split(',')
    conf.dir_modules_private = parser.get('paths', 'modules_private')
    conf.dir_manifests_generic = parser.get('paths', 'manifests_generic')
    conf.dir_manifests_private = parser.get('paths', 'manifests_private')
    conf.dir_hieradata_generic = parser.get('paths', 'hieradata_generic')
    conf.src_dir_hieradata_private = parser.get('paths', 'hieradata_private')
    conf.src_dir_files_private = parser.get('paths', 'files_private')
    conf.s3_access_key = parser.get('s3', 'access_key')
    conf.s3_secret_key = parser.get('s3', 'secret_key')
    conf.s3_bucket_name = parser.get('s3', 'bucket_name')
    conf.s3_host = parser.get('s3', 'host')
    conf.s3_port = int(parser.get('s3', 'port'))
    conf.sftp_hosts = parser.get('sftp', 'hosts').split(',')
    conf.sftp_username = parser.get('sftp', 'username')
    conf.sftp_private_key = parser.get('sftp', 'private_key')
    conf.posix_file_mode = int(parser.get('posix', 'file_mode'), 8)
    conf.posix_dir_mode = int(parser.get('posix', 'dir_mode'), 8)

def parse_args():
    """Parses CLI args, then set debug flag and configuration file path in
       runtime configuration accordingly, and returns the args."""
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--debug',
                        help='Enable debug mode',
                        action='store_true')
    parser.add_argument('-c', '--conf',
                        help='Path to the configuration file',
                        nargs='?',
                        default='/etc/hpc-config/push.conf')
    parser.add_argument('-e', '--environment',
                        help='Name of the pushed environment',
                        nargs='?')
    parser.add_argument('-V', '--version',
                        help='Version of the pushed config',
                        nargs='?')
    parser.add_argument('--full-tmp-cleanup',
                        help='Full tmp dir cleanup.',
                        action='store_true')
    parser.add_argument('-l', '--list',
                        help='List pushed environments.',
                        action='store_true')
    parser.add_argument('--enable-python-warnings',
                        help="Don't hide some Python warnings.",
                        action='store_true')
    args = parser.parse_args()

    if args.debug:
        conf.debug = True
    if args.conf:
        conf.conf_file = args.conf

    return args

def setup_warnings(enable=False):
    if not enable:
        warnings.filterwarnings("ignore", category=DeprecationWarning)
        warnings.filterwarnings("ignore", category=FutureWarning)

def setup_logger():

    if conf.debug is True:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

def override_conf(args):
    """Override configuration files parameters with args values."""
    if args.environment:
        conf.environment = args.environment
    if args.version:
        conf.version = args.version
    if args.full_tmp_cleanup:
        conf.full_tmp_cleanup = True
    if args.list:
        conf.list_environments = True


def init_tmpd():
    """Create tmp directory used to build areas archives, areas private files
       hierarchy and perform on-the-fly re-encryptions.

       This procedures also copies the private hiera repository to this
       temporary directory in order to avoid later on-the-fly re-encryption of
       area eyaml files into the sysadmin local copy, which would lead to local
       modifications of their Git repositories.

       The procedures also set some conf.dir_* parameters appropriately.
    """
    if not os.path.isdir(conf.dir_tmp):
        os.makedirs(conf.dir_tmp)
    conf.dir_tmp_gen = tempfile.mkdtemp(dir=conf.dir_tmp)
    conf.dir_tmp_keys = os.path.join(conf.dir_tmp_gen, 'keys')

    conf.dir_hieradata_private = os.path.join(conf.dir_tmp_gen,
                                              'private_build',
                                              conf.src_dir_hieradata_private)
    conf.dir_files_private = os.path.join(conf.dir_tmp_gen,
                                          'private_build',
                                          'files')
    shutil.copytree(conf.src_dir_hieradata_private,
                    conf.dir_hieradata_private,
                    copy_function=conf_copy)


def get_area_decrypt_password(area):
    """This function decrypt and parse the cluster_decrypt_password of a given
       area.

       This function can only work _before_ eyaml files are re-encrypted with
       reencrypt_area_eyaml_file()."""
    if area in _area_passwords_cache.keys():
        return _area_passwords_cache[area]

    # get cluster_decrypt_password from area yaml file using eyaml and system
    # keys:
    #   eyaml decrypt --file $hieradata/$cluster/areas/$area.yaml
    area_yaml_path = os.path.join(conf.dir_hieradata_private, conf.cluster,
                                  'areas', area + '.yaml')
    cmd = ['eyaml', 'decrypt', '--file', area_yaml_path]
    eyaml_run = subprocess.check_output(cmd)
    area_yaml = yaml.load(eyaml_run)
    _area_passwords_cache[area] = area_yaml['cluster_decrypt_password']
    return _area_passwords_cache[area]


def encrypt_file(infile, outfile, key):
    """Run openssl to encrypt infile with AES key."""
    cmd = ['openssl', 'aes-256-cbc', '-md', 'sha256',
           '-in', infile,
           '-out', outfile,
           '-k', key ]
    subprocess.check_call(cmd)


def decrypt_file(infile, outfile, key):
    """Run openssl to decrypt infile with AES key."""
    cmd = ['openssl', 'aes-256-cbc', '-md', 'sha256', '-d',
           '-in', infile,
           '-out', outfile,
           '-k', key ]
    subprocess.check_call(cmd)


def decrypt_extract_eyaml_keys():
    """This procedures decrypt and extract all areas (except main area) eyaml
       encryption keys. These keys will be required later by
       reencrypt_area_eyaml_file() to reencrypt the area eyaml file."""
    main_area_encoding_key = get_area_decrypt_password(conf.main_area)
    for area in conf.areas:
        if area != conf.main_area:
            # decrypt other area eyaml keys archive from source files into tmpdir
            other_area_keys_dir = os.path.join(conf.src_dir_files_private, conf.main_area, 'eyaml', area)
            other_area_keys_path_in = os.path.join(other_area_keys_dir, 'keys.tar.xz.enc')
            other_area_keys_path_out = os.path.join(conf.dir_tmp_keys, area, 'keys.tar.xz')
            logger.debug("decrypt/extract eyaml keys %s of area %s", other_area_keys_path_in, area)
            os.makedirs(os.path.join(conf.dir_tmp_keys, area))
            decrypt_file(other_area_keys_path_in, other_area_keys_path_out,main_area_encoding_key)
            # decrypt: openssl aes-256-cbc -md sha256 -in $IN -out $OUT -key $KEY
            # encrypt: openssl aes-256-cbc -md sha256 -d -in $IN -out $OUT -key $KEY
            other_area_keys_arch = tarfile.open(other_area_keys_path_out, mode='r')
            other_area_keys_arch.extractall(path=os.path.join(conf.dir_tmp_keys, area))


def reencrypt_area_eyaml_file(area):
    """Reencrypt the area eyaml file with the area eyaml keys previously
       extracted by decrypt_extract_eyaml_keys()."""
    area_eyaml_file_path   = os.path.join(conf.dir_hieradata_private, conf.cluster,
                                         'areas', area + '.yaml')
    area_eyaml_private_key = os.path.join(conf.dir_tmp_keys, area,
                                          'keys', 'private_key.pkcs7.pem')
    area_eyaml_public_key  = os.path.join(conf.dir_tmp_keys, area,
                                          'keys', 'public_key.pkcs7.pem')
    logger.debug("reencrypt %s with area %s eyaml keys",
                 area_eyaml_file_path, area)

    # decrypt the file using system keys with:
    #   eyaml decrypt --eyaml $FILE
    cmd = ['eyaml', 'decrypt', '--eyaml', area_eyaml_file_path]
    eyaml_run = subprocess.check_output(cmd, universal_newlines=True)
    # rewrite the buffered output to the file
    with open(area_eyaml_file_path, 'w') as fh:
        fh.write(eyaml_run)

    # recrypt the file with:
    #   eyaml recrypt --pkcs7-private-key $PRIV --pkcs7-public-key $PUB $FILE
    cmd = ['eyaml', 'recrypt',
           '--pkcs7-private-key', area_eyaml_private_key,
           '--pkcs7-public-key', area_eyaml_public_key,
           area_eyaml_file_path]
    subprocess.check_call(cmd)


def build_tarballs():
    """Build the tarballs for all areas."""
    for area in conf.areas:
        build_tarball(area)


def build_tarball(area):

    logger.info("creating archive %s", conf.archive_path(area))
    os.makedirs(os.path.dirname(conf.archive_path(area)))
    tar = tarfile.open(name=conf.archive_path(area), mode='w:xz', dereference=True)

    # generic modules
    seen_modules = []
    for modulesdir in conf.dir_modules_generic:

        if os.path.exists(modulesdir) and \
           os.path.isdir(modulesdir):

            # detect and raise error in case of module conflict
            new_modules = os.listdir(modulesdir)
            intersect = list(set(seen_modules) & set(new_modules))
            if len(intersect):
                logger.error("modules conflict in %s: %s", modulesdir, str(intersect))
                sys.exit(1)
            seen_modules += new_modules

            logger.debug("adding generic modules dir %s: %s", modulesdir, str(new_modules))
            tar.add(modulesdir, arcname=os.path.join(conf.environment, 'modules_generic'))
        else:
            logger.warning("Configured generic modules dir is missing: '%s'",
                           modulesdir)

    # private modules
    if os.path.exists(conf.dir_modules_private) and \
       os.path.isdir(conf.dir_modules_private):
        logger.debug("adding private modules dir %s", conf.dir_modules_private)
        tar.add(conf.dir_modules_private, arcname=os.path.join(conf.environment, 'modules_private'))
    else:
        logger.warning("Configured private modules dir is missing: '%s'",
                       conf.dir_modules_private)
    # generic manifests
    if os.path.exists(conf.dir_manifests_generic) and \
       os.path.isdir(conf.dir_manifests_generic):
        logger.debug("adding generic manifests dir %s", conf.dir_manifests_generic)
        tar.add(conf.dir_manifests_generic, arcname=os.path.join(conf.environment, 'manifests'))
    else:
        logger.warning("Configured generic manifests dir is missing: '%s'",
                       conf.dir_manifests_generic)
    # private manifests
    if os.path.exists(conf.dir_manifests_private) and \
       os.path.isdir(conf.dir_manifests_private):
        logger.debug("adding private manifests dir %s", conf.dir_manifests_private)
        tar.add(conf.dir_manifests_private, arcname=os.path.join(conf.environment, 'manifests'))
    else:
        logger.warning("Configured private manifests dir is missing: '%s'",
                       conf.dir_manifests_private)
    # generic hieradata
    if os.path.exists(conf.dir_hieradata_generic) and \
       os.path.isdir(conf.dir_hieradata_generic):
        logger.debug("adding generic hieradata dir %s", conf.dir_hieradata_generic)
        tar.add(conf.dir_hieradata_generic, arcname=os.path.join(conf.environment, 'hieradata', 'generic'))
    else:
        logger.warning("Configured generic hieradata dir is missing: '%s'",
                       conf.dir_hieradata_generic)
    # private hieradata
    if os.path.exists(conf.dir_hieradata_private) and \
       os.path.isdir(conf.dir_hieradata_private):
        logger.debug("adding private hieradata dir %s", conf.dir_hieradata_private)
        # The area tarbal must contain these files:
        #   $dir_hieradata_private/*.yaml
        #   $dir_hieradata_private/$cluster/*.yaml
        #   $dir_hieradata_private/$cluster/roles/*.yaml
        #   $dir_hieradata_private/$cluster/areas/$area.yaml
        base_arcname = os.path.join(conf.environment, 'hieradata', 'private')
        arch_files = \
          glob.glob(os.path.join(conf.dir_hieradata_private, '*.yaml')) + \
          glob.glob(os.path.join(conf.dir_hieradata_private, conf.cluster, '*.yaml')) + \
          glob.glob(os.path.join(conf.dir_hieradata_private, conf.cluster, 'roles', '*.yaml')) + \
          [ os.path.join(conf.dir_hieradata_private, conf.cluster, 'areas', area + '.yaml') ]
        if area != conf.main_area:
            # re-enc area yaml file
            reencrypt_area_eyaml_file(area)
        for arch_file in arch_files:
            # remove dir_hieradata_private from arch_file
            subpath = arch_file[len(conf.dir_hieradata_private)+1:]
            tar.add(arch_file,
                    arcname=os.path.join(base_arcname, subpath),
                    recursive=False)
    else:
        logger.warning("Configured private hieradata dir is missing: '%s'",
                       conf.dir_hieradata_private)

    logger.debug("adding environment conf")
    tar.add(conf.conf_environment_gen, arcname=os.path.join(conf.environment, conf.conf_environment))

    tar.close()

def gen_env_conf():

    with open(conf.conf_environment_gen, 'w+') as env_f:
        env_f.write("modulepath=modules_private:modules_generic\n")
        env_f.write("manifest=manifests/cluster.pp\n")


def reenc_file(encrypted_file, source_key, dest_key):
    logger.debug("reencrypt private file %s with cluster_decrypt_password",
                 encrypted_file)
    # decrypt and re-encrypt file
    unencrypted_file = encrypted_file[:-4]
    decrypt_file(encrypted_file, unencrypted_file, source_key)
    encrypt_file(unencrypted_file, encrypted_file, dest_key)
    os.remove(unencrypted_file)
    return True

def copy_reenc_private_files():
    """This procedure merges the cluster and $area private files directories
       into one $area hierarchy. Then, for all areas except the main one, it
       searches encoded files to reencrypt them using the
       cluster_decrypt_password of this area.
    """
    # Find all files in conf.dir_files_private, decrypt them with main area
    # cluster_decrypt_passwd and re-encrypt them with cluster_decrypt_password of
    # each other area
    pool = Pool()
    results = []
    master_key = get_area_decrypt_password(conf.main_area)
    for area in conf.areas:
        # copy file in area subdir
        private_files_area_build_dir = os.path.join(conf.dir_files_private, area)
        for subdir in ['cluster', area]:
            conf_copy(os.path.join(conf.src_dir_files_private, subdir),
                      private_files_area_build_dir)
        if area != conf.main_area:
            # find all encrypted files
            encrypted_files=[]
            for root, dirnames, filenames in os.walk(private_files_area_build_dir):
                for filename in fnmatch.filter(filenames, '*.enc'):
                      encrypted_files.append(os.path.join(root, filename))
            # read area cluster_decrypt_password
            area_key = get_area_decrypt_password(area)
            for encrypted_file in encrypted_files:
                results.append(pool.apply_async(
                    reenc_file,
                    [encrypted_file, master_key, area_key]
                ))
    pool.close()
    pool.join()
    success = 0
    errors = 0
    for result in results:
        function_return = result.get()
        if function_return:
            success += 1
        else:
            errors += 1
    logger.info("Reenc Files: Reencrypted %d files, %d errors", success, errors)


def _push_posix():

    logger.info("posix push: pushing data in %s", conf.destination)

    if not os.path.isdir(conf.destination):
        logger.debug("posix push: create destination dir %s", conf.destination)
        os.makedirs(conf.destination, exist_ok=True)

    for area in conf.areas:
        logger.debug("posix push: copying area %s tarball", area)
        area_dest = os.path.join(conf.destination, area)
        os.makedirs(area_dest, exist_ok=True)
        shutil.copy(conf.archive_path(area), area_dest)

    dir_files = os.path.join(conf.destination, 'files')
    if os.path.isdir(dir_files):
        logger.debug("posix push: removing push private files dir %s", dir_files)
        shutil.rmtree(dir_files)

    logger.debug("posix push: copying private files")
    # copytree() default copy_function is shutil.copy2() which does not manage
    # directories. When the file is a symlink, copytree() resolve the link and
    # directly call copy2() with the target of the link. It fails with errno 21
    # when the target is a directory. To avoid this bug, use an alternate copy
    # function conf_copy() to properly handle symlinks on directories.
    shutil.copytree(conf.dir_files_private, dir_files, copy_function=conf_copy)
    logger.debug("posix push: copying puppet conf")
    shutil.copy(conf.conf_puppet, conf.destination)
    logger.debug("posix push: copying hiera conf")
    shutil.copy(conf.conf_hiera, conf.destination)
    logger.debug("posix push: copying private cluster nodes description")
    shutil.copy(conf.nodes_private, conf.destination)

    # Set permissions
    for root, dirs, files in os.walk(conf.destination):
        for dir_name in dirs:
            os.chmod(os.path.join(root, dir_name), conf.posix_dir_mode)
        for file_name in files:
            os.chmod(os.path.join(root, file_name), conf.posix_file_mode)


def _list_upload_file_paths(source_path):
    logger.debug('Upload path: %s', source_path)
    # List files to upload
    upload_file_paths = []
    if os.path.isfile(source_path):
        relative_path = os.path.basename(source_path)
        upload_file_paths.append(relative_path)
    for (current_dir, subdirs, filenames) in os.walk(source_path, followlinks=True):
        for filename in filenames:
            absolute_path = os.path.join(current_dir, filename)
            # remove the source path and first /
            relative_path = absolute_path[(len(source_path)+1):]
            upload_file_paths.append(relative_path)
    return upload_file_paths

def _get_full_paths(source_path, destination_path, file_path):
    # Determine file paths
    if os.path.isfile(source_path):
        source_file_path = source_path
    else:
        source_file_path = os.path.join(source_path, file_path)
    logger.debug("Source file path is: %s (%s, %s)", source_file_path, source_path, file_path)
    dest_file_path = os.path.join(destination_path, file_path)
    logger.debug("Dest file path is: %s (%s, %s)", dest_file_path, destination_path, file_path)
    return source_file_path, dest_file_path


def _s3_upload_file(source_file_path,
                    bucket,
                    destination_file_path,
                    object_md5s=None):
    #max size in bytes before uploading in parts. between 1 and 5 GB recommended
    max_size = 20 * 1000 * 1000
    #size of parts when uploading in parts
    part_size = 6 * 1000 * 1000

    if object_md5s is None:
        oject_md5s = {}

    # Check if the file has changed
    if destination_file_path in object_md5s.keys():
        remote_md5 = object_md5s[destination_file_path]
    else:
        remote_key = bucket.get_key(destination_file_path)
        if remote_key is not None:
            remote_md5 = remote_key.etag[1:-1]
        else:
            remote_md5 = None
    local_md5 = hashlib.md5(open(source_file_path, 'rb').read()).hexdigest()
    if remote_md5 == local_md5:
        logger.debug("S3 upload: MD5 Match for file %s", source_file_path)
        return
    else:
        logger.debug("S3 upload: MD5 Mismatch for file %s (%s != %s)",
                     source_file_path,
                     remote_md5,
                     local_md5)


    # Determine upload method
    filesize = os.path.getsize(source_file_path)
    if filesize > max_size:
        logger.debug("S3 upload: multipart upload for %s", source_file_path)
        mp = bucket.initiate_multipart_upload(destination_file_path,
                                              policy='public-read')
        fp = open(source_file_path, 'rb')
        fp_num = 0
        while fp.tell() < filesize:
            fp_num += 1
            logger.debug("S3 upload: uploading part %i", fp_num)
            mp.upload_part_from_file(fp, fp_num, size=part_size)

        mp.complete_upload()
    else:
        logger.debug("S3 upload: singlepart upload for %s", source_file_path)
        k = boto.s3.key.Key(bucket)
        k.key = destination_file_path
        bytes_written = k.set_contents_from_filename(source_file_path,
                                                     policy='public-read')
        logger.debug("S3 upload: %d/%d bytes written for %s",
                     bytes_written, filesize, source_file_path)


def _s3_upload(source_path,
               bucket,
               destination_path,
               clean=True,
               object_md5s=None):
    upload_file_paths = _list_upload_file_paths(source_path)

    if object_md5s is None:
        object_md5s = {}
    pool = ThreadPool()
    results = {}

    dirs = []
    touched_objects = []


    # Upload the files
    for file_path in upload_file_paths:
        source_file_path, dest_file_path = _get_full_paths(
            source_path, destination_path, file_path)

        # Create remote directory if necessary
        dest_dir_name = os.path.dirname(dest_file_path) + "/"
        while dest_dir_name not in dirs \
                and dest_dir_name not in object_md5s.keys() \
                and bucket.get_key(dest_dir_name) is None:
            logger.debug("S3 upload: Creating directory %s", dest_dir_name)
            dest_dir = bucket.new_key(dest_dir_name)
            dest_dir.set_contents_from_string('', policy='public-read')
            dirs.append(dest_dir_name)
            touched_objects.append(dest_dir_name)
            # Continue with parent
            dest_dir_name = os.path.dirname(dest_dir_name[:-1]) + "/"

        results[dest_file_path] = pool.apply_async(
            _s3_upload_file,
            [source_file_path, bucket, dest_file_path, object_md5s]
        )
        touched_objects.append(dest_file_path)

    pool.close()
    finished = 0
    while finished < len(results):
        finished = 0
        for result in results.values():
            if result.ready():
                finished += 1
        logger.info("S3 push: Transfered files %d/%d", finished, len(results))
        time.sleep(1)
    for dest_file_path, result in results.items():
        result.get()
    pool.join()
    return touched_objects

def _s3_list_md5(bucket, prefix):
    keys = bucket.list(prefix=prefix)
    md5s = {}
    for key in keys:
        md5s[key.name] = key.etag[1:-1]
    return md5s

def _s3_remove_old_objects(bucket, old_object_md5s, new_objects):
    """
        Remove all objects from the old list not in the new list.
    """
    key_names = []
    for object_name in old_object_md5s.keys():
        if object_name not in new_objects:
            if not object_name.endswith("/"):
                logger.debug("S3 push: Removing old file: %s", object_name)
                key_names.append(object_name)
                continue
            empty=True
            for file_name in new_objects:
                if file_name.startswith(object_name):
                    empty=False
            if empty:
                logger.debug("S3 push: Removing old dir: %s", object_name)
                key_names.append(object_name)
    logger.info("S3 push: %d files to remove.", len(key_names))
    result = bucket.delete_keys(key_names)
    error_count = len(result.errors)
    deleted_count = len(result.deleted)
    logger.debug("S3 push: %d deleted, %s errors", deleted_count, error_count)
    if error_count > 0:
        logger.error("S3 push: Failed to delete %d old keys", error_count)

def _s3_rmrf(bucket, prefix):
    keys = bucket.list(prefix=prefix)
    pool = ThreadPool()
    results = []
    counter = 1
    key_group = []
    for key in keys:
        key_group.append(key)
        counter += 1
        if counter == 1000:
            results.append(
                pool.apply_async(bucket.delete_keys, [key_group])
            )
            key_group=[]
            counter = 1
    results.append(
        pool.apply_async(bucket.delete_keys, [key_group])
    )
    pool.close()
    pool.join()
    deleted = 0
    errors = 0
    for result in results:
        report = result.get()
        deleted += len(report.deleted)
        errors += len(report.errors)
    logger.info("S3 push: Deleted %d keys, %d errors", deleted, errors)

def _bucket_conn_s3(conf):
    """Connect to S3 server and return the bucket."""
    conn = boto.connect_s3(
        aws_access_key_id=conf.s3_access_key,
        aws_secret_access_key=conf.s3_secret_key,
        host=conf.s3_host,
        port=conf.s3_port,
        is_secure=False,
        calling_format=boto.s3.connection.OrdinaryCallingFormat(),
    )
    bucket = conn.get_bucket(conf.s3_bucket_name)
    bucket.set_acl('public-read')
    return bucket

def _push_s3():
    logger.info("S3 push: pushing data in bucket %s", conf.s3_bucket_name)

    bucket = _bucket_conn_s3(conf)

    logger.info("S3 push: get remote objects list")
    obj_md5s = _s3_list_md5(bucket, conf.destination)

    touched_objects = []

    for area in conf.areas:
        logger.info("S3 push: copying area %s tarball", area)
        area_dest = os.path.join(conf.destination, area)
        lst = _s3_upload(conf.archive_path(area), bucket, area_dest, object_md5s=obj_md5s)
        touched_objects = list(set(touched_objects + lst))


    logger.info("S3 push: copying private files")
    dir_files = os.path.join(conf.destination, 'files')
    lst = _s3_upload(conf.dir_files_private, bucket, dir_files, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))
    logger.info("S3 push: copying puppet conf")
    lst = _s3_upload(conf.conf_puppet, bucket, conf.destination, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))
    logger.info("S3 push: copying hiera conf")
    lst = _s3_upload(conf.conf_hiera, bucket, conf.destination, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))
    logger.info("S3 push: copying private cluster nodes description")
    lst = _s3_upload(conf.nodes_private, bucket, conf.destination, object_md5s=obj_md5s)
    touched_objects = list(set(touched_objects + lst))

    logger.info("S3 push: Removing old files")
    _s3_remove_old_objects(bucket, obj_md5s, touched_objects)

def _sftp_is_dir(sftp_client, path):
    if sftp_client not in _sftp_host_directories.keys():
        _sftp_host_directories[sftp_client] = []
    if path in _sftp_host_directories[sftp_client]:
        return True
    try:
        is_dir = stat.S_ISDIR(sftp_client.stat(path).st_mode)
    except FileNotFoundError:
        is_dir = False
    if is_dir:
        _sftp_host_directories[sftp_client].append(path)
    return is_dir

def _sftp_list_children(sftp_client, path):
    children_attr = sftp_client.listdir_attr(path)
    files = []
    directories = []
    for child_attr in children_attr:
        name = child_attr.filename
        if stat.S_ISDIR(child_attr.st_mode):
            directories.append(name)
            # cache result
            if sftp_client not in _sftp_host_directories.keys():
                _sftp_host_directories[sftp_client] = []
            full_path = os.path.join(path, name)
            if full_path not in _sftp_host_directories[sftp_client]:
                _sftp_host_directories[sftp_client].append(name)
        else:
            files.append(name)
    return directories, files

def _sftp_rmrf(sftp_client, path):
    _sftp_host_directories[sftp_client] = []
    if _sftp_is_dir(sftp_client, path):
        directories, files = _sftp_list_children(sftp_client, path)
        for directory in directories:
            directory_path = os.path.join(path, directory)
            _sftp_rmrf(sftp_client, directory_path)
        for filename in files:
            file_path = os.path.join(path, filename)
            sftp_client.remove(file_path)
        sftp_client.rmdir(path)
    else:
        try:
            sftp_client.remove(path)
            logger.debug("SFTP: Removing: %s" % path)
        except FileNotFoundError:
            logger.debug("SFTP: Try to remove a missing file: %s" % path)
    _sftp_host_directories[sftp_client] = []

def _sftp_mkdir(sftp_client, path, mode=0o755):
    if _sftp_is_dir(sftp_client, path):
        return
    else:
        parent = os.path.dirname(path[:-1])
        _sftp_mkdir(sftp_client, parent, mode)
    sftp_client.mkdir(path)
    sftp_client.chmod(path, mode)

def _sftp_upload(source_path, sftp_client, destination_path, clean=True):
    upload_file_paths = _list_upload_file_paths(source_path)

    for file_path in upload_file_paths:
        source_file_path, dest_file_path = _get_full_paths(
            source_path, destination_path, file_path)
        # Create remote directory if necessary
        dest_dir_name = os.path.dirname(dest_file_path)
        _sftp_mkdir(sftp_client, dest_dir_name)
        # Upload
        sftp_client.put(source_file_path, dest_file_path, confirm=False)
        # Set Perms
        sftp_client.chmod(dest_file_path, 0o644)

def _sftp_connect(host, conf, verb):
    """Connect to SFTP server host. Verb is used in prefix of log messages."""
    key = paramiko.RSAKey.from_private_key_file(conf.sftp_private_key)
    username = conf.sftp_username

    try:
        transport = paramiko.Transport((host, 22))
        transport.connect(username=username, pkey=key)
    except socket.gaierror as e:
        logger.error("SFTP %s: Failed to connect to host %s", verb, host)
        logger.info("SFTP %s: Connection error: %s.", verb, e)
        return
    except paramiko.ssh_exception.SSHException as e:
        logger.error("SFTP %s: SSH failed to %s@%s", verb, username, host)
        logger.info("SFTP %s: SSH error: %s.", verb, e)
        return
    return paramiko.SFTPClient.from_transport(transport)

def _sftp_push_host(host, conf):

    sftp_client = _sftp_connect(host, conf, verb='push')

    logger.debug("SFTP push: Cleaning destination %s", conf.destination)
    _sftp_rmrf(sftp_client, conf.destination)

    for area in conf.areas:
        logger.debug("SFTP push: copying area %s tarball", area)
        area_dest = os.path.join(conf.destination, area)
        _sftp_upload(conf.archive_path(area), sftp_client, area_dest)

    logger.debug("SFTP push: copying private files")
    dir_files = os.path.join(conf.destination, 'files')
    _sftp_upload(conf.dir_files_private, sftp_client, dir_files)
    logger.debug("SFTP push: copying puppet conf")
    _sftp_upload(conf.conf_puppet, sftp_client, conf.destination)
    logger.debug("SFTP push: copying hiera conf")
    _sftp_upload(conf.conf_hiera, sftp_client, conf.destination)
    logger.debug("SFTP push: copying private cluster nodes description")
    _sftp_upload(conf.nodes_private, sftp_client, conf.destination)


def _push_sftp():
    logger.info("SFTP push: pushing data on hosts %s", conf.sftp_hosts)

    pool = Pool()
    results = {}
    for host in conf.sftp_hosts:
       results[host] = pool.apply_async(_sftp_push_host, [host, conf])
    pool.close()
    finished = 0
    while finished < len(results):
        finished = 0
        for result in results.values():
            if result.ready():
                finished += 1
        logger.info("SFTP push: Finished host %d/%d", finished, len(results))
        time.sleep(1)
    for host, result in results.items():
        result.get()
    pool.join()

def push():

    if conf.mode == 'posix':
        _push_posix()
    elif conf.mode == 's3':
        _push_s3()
    elif conf.mode == 'sftp':
        _push_sftp()
    else:
        logger.error("unsupported push mode %s", conf.mode)
        sys.exit(1)

def _formatted_list_results(envs):
    """Compose multiline string of formatted results w/ list comprehension
       on a list of tuples (filename, mtime as string)."""
    return '\n'.join([ "  - %-20s [%s]" % (env[0], env[1])
                       for env in envs ])

def _list_environments_posix():
    """List pushed environments in POSIX directory."""

    logger.info("posix list: in %s", conf.destination_root)

    if not os.path.isdir(conf.destination_root):
        logger.info("posix list: no environment")
        return

    env_dirs = sorted(os.listdir(conf.destination_root))
    # build list of envs tuples (name, mtime)
    envs = [ (env_dir,
              datetime.utcfromtimestamp(
                os.stat(os.path.join(conf.destination_root, env_dir)).st_mtime) \
                .strftime('%Y-%m-%d %H:%M:%S'))
             for env_dir in env_dirs ]
    result_s = _formatted_list_results(envs)

    logger.info("posix list: available environment:\n%s", result_s)

def _list_environments_s3():
    """List pushed environments in Ceph/S3 Bucket."""

    logger.info("S3 list: listing environments in bucket %s", conf.s3_bucket_name)

    bucket = _bucket_conn_s3(conf)

    logger.info("S3 list: get remote objects list")
    objs = bucket.list(prefix=conf.destination_root)
    envs = []
    for obj in objs:
        name_members = obj.name.split('/')
        if len(name_members) == 3:
            envs.append((name_members[1], obj.last_modified))
    result_s = _formatted_list_results(envs)

    logger.info("S3 list: available environments:\n%s", result_s)

def _sftp_list_host(host, conf):
    """Returns a list of tuples with filename and mtime of pushed environments
       on a specific SFTP server."""
    sftp_client = _sftp_connect(host, conf, verb='list')

    results = []
    for attr in sftp_client.listdir_iter(conf.destination_root):
        results.append((attr.filename,
                        datetime.utcfromtimestamp(attr.st_mtime) \
                          .strftime('%Y-%m-%d %H:%M:%S')))

    return sorted(results)

def _list_environments_sftp():
    """List pushed environment asynchronously on all SFTP servers."""

    logger.info("SFTP list: list environments on hosts %s", conf.sftp_hosts)

    pool = Pool()
    results = {}
    for host in conf.sftp_hosts:
       results[host] = pool.apply_async(_sftp_list_host, [host, conf])
    pool.close()
    finished = 0
    while finished < len(results):
        finished = 0
        for result in results.values():
            if result.ready():
                finished += 1
        logger.info("SFTP list: Finished host %d/%d", finished, len(results))
        time.sleep(1)
    # Build a hash with string of formatted result as keys and list of hosts
    # having this result as values.
    all_hosts_envs = {}
    for host, result in results.items():
        envs = result.get()  # list of tuples
        result_s = _formatted_list_results(envs)
        if result_s in all_hosts_envs:
            all_hosts_envs[result_s].append(host)
        else:
            all_hosts_envs[result_s] = [ host ]
    pool.join()
    for envs_s, hosts in all_hosts_envs.items():
        logger.info("SFTP list: hosts: %s environments:\n%s", ','.join(hosts), envs_s)

def list_environments():
    """List pushed environments by calling the appropriate function depending
       on the storage backend."""
    if conf.mode == 'posix':
        _list_environments_posix()
    elif conf.mode == 'sftp':
        _list_environments_sftp()
    elif conf.mode == 's3':
        _list_environments_s3()
    else:
        logger.error("environments listing unsupported for mode %s", conf.mode)
        sys.exit(1)

def cleanup_run():
    """Remove the run tmp dir."""

    logger.debug("removing run tmp dir %s", conf.dir_tmp_gen)
    shutil.rmtree(conf.dir_tmp_gen)

def cleanup_full():
    """Remove the full app tmp dir."""
    if not os.path.isdir(conf.dir_tmp):
        logger.info("app tmp dir %s does not exists, nothing to remove.", conf.dir_tmp)
    else:
        logger.info("removing app tmp dir %s", conf.dir_tmp)
        shutil.rmtree(conf.dir_tmp)

def main():

    #
    # init
    #
    args = parse_args()
    setup_warnings(args.enable_python_warnings)
    setup_logger()
    parse_conf()
    override_conf(args)
    conf.dump()

    #
    # run
    #
    if conf.full_tmp_cleanup:
        cleanup_full()
    elif conf.list_environments:
        list_environments()
    else:
        init_tmpd()
        decrypt_extract_eyaml_keys()
        copy_reenc_private_files()
        gen_env_conf()
        build_tarballs()
        push()
        cleanup_run()

if __name__ == '__main__':
    main()
